1. FedBoost: A Communication-Efficient Algorithm for Federated Learning.
http://proceedings.mlr.press/v119/hamer20a.html


    Communication cost is often a bottleneck in federated learning and other client-based distributed learning scenarios. To overcome this, several gradient compression and model compression algorithms have been proposed. In this work, we propose an alternative approach whereby an ensemble of pre-trained base predictors is trained via federated learning. This method allows for training a model which may otherwise surpass the communication bandwidth and storage capacity of the clients to be learned with on-device data through federated learning. Motivated by language modeling, we prove the optimality of ensemble methods for density estimation for standard empirical risk minimization and agnostic risk minimization. We provide communication-efficient ensemble algorithms for federated learning, where per-round communication cost is independent of the size of the ensemble. Furthermore, unlike works on gradient compression, our proposed approach reduces the communication cost of both server-to-client and client-to-server communication.
  


通信成本通常是联邦学习和其他基于客户机的分布式学习场景的瓶颈。为了克服这一问题，提出了几种梯度压缩和模型压缩算法。在这项工作中，我们提出了一种替代的方法，通过联合学习训练一个预先训练的基础预测器的集合。这种方法允许训练一个模型，否则这个模型可能会超过要通过联合学习的设备上数据来学习的客户端的通信带宽和存储容量。基于语言建模的动机，我们证明了用于密度估计的集成方法对于标准经验风险最小化和不可知论风险最小化的最优性。我们为联邦学习提供了通信效率高的集成算法，其中每轮通信成本与集成的大小无关。此外，与梯度压缩不同，我们提出的方法降低了服务器到客户端和客户到服务器的通信成本。


2. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning.
http://proceedings.mlr.press/v119/karimireddy20a.html


    Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client’s data results in a ‘drift’ in the local updates resulting in poor performance. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the ‘client drift’. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client’s data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.
  


联邦学习是现代大规模机器学习中的一个关键场景，数据分布在大量的客户机上，任务是学习一个集中的模型，而不传输客户机数据。该设置中使用的标准优化算法为联邦平均(federalaverage, FedAvg)，因为其通信成本较低。我们对FedAvg的收敛性进行了严格的描述，并证明了客户端数据的异质性(非iid性)会导致局部更新中的“漂移”，从而导致性能低下。作为一种解决方案，我们提出了一种新的算法(SCAFFOLD)，它使用控制变量(方差减少)来纠正“客户漂移”。我们证明了SCAFFOLD所需的通信轮数显著减少，并且不受数据异质性或客户端抽样的影响。此外，我们表明(对于二次函数)SCAFFOLD可以利用客户数据的相似性，从而产生更快的收敛速度。后者是量化局部步骤在分布式优化中的作用的第一个结果。


3. Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization.
http://proceedings.mlr.press/v119/li20g.html


    Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. In this paper, we remedy this situation and propose the first \emph{accelerated compressed gradient descent (ACGD)} methods. In the single machine regime, we prove that ACGD enjoys the rate $O\Big((1+\omega)\sqrt{\frac{L}{\mu}}\log \frac{1}{\epsilon}\Big)$ for $\mu$-strongly convex problems and $O\Big((1+\omega)\sqrt{\frac{L}{\epsilon}}\Big)$ for convex problems, respectively, where $\omega$ is the compression parameter. Our results improve upon the existing non-accelerated rates $O\Big((1+\omega)\frac{L}{\mu}\log \frac{1}{\epsilon}\Big)$ and $O\Big((1+\omega)\frac{L}{\epsilon}\Big)$, respectively, and recover the optimal rates of accelerated gradient descent as a special case when no compression ($\omega=0$) is applied. We further propose a distributed variant of ACGD (called ADIANA) and prove the convergence rate $\widetilde{O}\Big(\omega+\sqrt{\frac{L}{\mu}}+\sqrt{\big(\frac{\omega}{n}+\sqrt{\frac{\omega}{n}}\big)\frac{\omega L}{\mu}}\Big)$, where $n$ is the number of devices/workers and $\widetilde{O}$ hides the logarithmic factor $\log \frac{1}{\epsilon}$. This improves upon the previous best result $\widetilde{O}\Big(\omega + \frac{L}{\mu}+\frac{\omega L}{n\mu} \Big)$ achieved by the DIANA method of Mishchenko et al. (2019). Finally, we conduct several experiments on real-world datasets which corroborate our theoretical results and confirm the practical superiority of our accelerated methods.
  


由于分布式和联合学习问题的通信成本较高，依赖于通信信息压缩的方法越来越受欢迎。虽然在其他环境中，执行最好的梯度类型方法总是依赖于某种形式的加速/动量来减少迭代的数量，但是没有方法能够同时结合梯度压缩和加速的好处。在本文中，我们修正了这种情况，并提出了第一种加速压缩梯度下降(ACGD)方法。在单机状态下，我们证明了ACGD对于$ mu$-强凸问题具有$O大((1+\ ω)\sqrt{frac{L}{mu}}\log \frac{1}{epsilon}}\Big)$，对于凸问题具有$O大((1+\ ω)\sqrt{frac{L}{\epsilon}}\Big)$，其中$ omega$为压缩参数。我们的结果分别改进了现有的非加速速率$O\Big((1+\omega)\frac{L}{mu}\log \frac{1}{epsilon}\Big)$和$O\Big((1+\omega)\frac{L}{\epsilon}\Big)$，并恢复了无压缩($ omega=0$)时的最优加速梯度下降速率。我们进一步提出一个分布式的变体ACGD(称为ADIANA)并证明收敛速度$ \ widetilde {O} \大\ω+ \√{\压裂{L}{\μ}}+ \ sqrt{\大(\压裂{\ω}{n} + \ sqrt{\压裂{\ω}{n}} \大)\压裂{\ωL}{\μ}}\大),美元在$ n是设备的数量/工人和美元\ widetilde {O}隐藏了对数因子\ log \美元压裂{1}{\ε}$。这改进了Mishchenko等人(2019)通过DIANA方法获得的先前最佳结果$\ widtilde {O}\Big(\omega +\frac{L}{\mu}+\frac{omega L}{n\mu} \Big)$。最后，我们在真实数据集上进行了几个实验，证实了我们的理论结果，并证实了我们的加速方法的实际优越性。


4. From Local SGD to Local Fixed-Point Methods for Federated Learning.
http://proceedings.mlr.press/v119/malinovskiy20a.html


    Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.
  


大多数求解优化问题或寻找凸凹函数鞍点的算法都是不动点算法。在这个工作中，我们考虑在一个分布的环境中寻找一个平均算子的不动点的一般问题，或一个近似的不动点。我们的工作是由联邦学习的需求所推动的。在这种情况下，每个本地运营商对在移动设备上本地完成的计算进行建模。我们研究了两种策略来实现这样的共识:一种基于固定数量的局部步骤，另一种基于随机计算。在这两种情况下，目标都是限制本地计算变量的通信，这通常是分布式框架的瓶颈。我们对这两种方法进行了收敛性分析，并进行了大量的实验，突出了我们的方法的好处。


5. FetchSGD: Communication-Efficient Federated Learning with Sketching.
http://proceedings.mlr.press/v119/rothchild20a.html


    Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation. In this paper we introduce a novel algorithm,called FetchSGD, to overcome these challenges. FetchSGD compresses model updates using a Count Sketch, and then takes advantage of the mergeability of sketches to combine model updates from many workers. A key insight in the design of FetchSGD is that, because the Count Sketch is linear, momentum and error accumulation can both be carried out within the sketch.This allows the algorithm to move momentum and error accumulation from clients to the central aggregator, overcoming the challenges of sparse client participation while still achieving high compression rates and good convergence. We prove that FetchSGD has favorable convergence guarantees, and we demonstrate its empirical effectiveness by training two residual networks and a transformer model.
  


现有的联合学习方法由于稀疏的客户端参与而存在通信瓶颈和收敛问题。在本文中，我们引入了一种名为FetchSGD的新算法来克服这些挑战。FetchSGD使用一个Count Sketch来压缩模型更新，然后利用草图的可合并性来组合来自多个worker的模型更新。FetchSGD设计的一个关键观点是，因为Count Sketch是线性的，动量和误差积累都可以在草图中进行。这使得算法可以将动量和错误积累从客户端转移到中央聚合器，克服了稀疏客户端参与的挑战，同时仍然实现高压缩率和良好的收敛性。我们证明了FetchSGD具有良好的收敛保证，并通过训练两个残差网络和一个变压器模型来证明其经验有效性。


6. Federated Learning with Only Positive Labels.
http://proceedings.mlr.press/v119/yu20f.html


    We consider learning a multi-class classification model in the federated setting, where each user has access to the positive data associated with only a single class. As a result, during each federated learning round, the users need to locally update the classifier without having access to the features and the model parameters for the negative classes. Thus, naively employing conventional decentralized learning such as distributed SGD or Federated Averaging may lead to trivial or extremely poor classifiers. In particular, for embedding based classifiers, all the class embeddings might collapse to a single point. To address this problem, we propose a generic framework for training with only positive labels, namely Federated Averaging with Spreadout (FedAwS), where the server imposes a geometric regularizer after each round to encourage classes to be spreadout in the embedding space. We show, both theoretically and empirically, that FedAwS can almost match the performance of conventional learning where users have access to negative labels. We further extend the proposed method to settings with large output spaces.
  


我们考虑在联邦设置中学习多类分类模型，其中每个用户只能访问与单个类相关的正数据。因此，在每一轮联邦学习中，用户需要局部更新分类器，而不需要访问负类的特征和模型参数。因此，天真地使用传统的分散学习，如分布式SGD或联邦平均，可能会导致微不足道或非常差的分类器。特别是，对于基于嵌入的分类器，所有的类嵌入可能会崩溃为一个点。为了解决这个问题，我们提出了一个只使用正标签进行训练的通用框架，即使用Spreadout的联邦平均(FedAwS)，其中服务器在每轮之后强加一个几何正则化器，以鼓励类在嵌入空间中展开。我们从理论和经验两方面证明，FedAwS几乎可以匹配传统学习的性能，用户可以访问负面标签。我们进一步将所提出的方法扩展到具有大输出空间的设置。


