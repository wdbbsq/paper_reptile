1. FedRec++: Lossless Federated Recommendation with Explicit Feedback.
https://ojs.aaai.org/index.php/AAAI/article/view/16546

Abstract
With the marriage of federated machine learning and recommender systems for privacy-aware preference modeling and personalization, there comes a new research branch called federated recommender systems aiming to build a recommendation model in a distributed way, i.e., each user is represented as a distributed client where his/her original rating data are not shared with the server or the other clients. Notice that, besides the sensitive information of a specific rating score assigned to a certain item by a user, the information of a user's rated set of items shall also be well protected. Some very recent works propose to randomly sample some unrated items for each user and then assign some virtual ratings, so that the server can not identify the scores and the set of rated items easily during the server-client interactions. However, the virtual ratings assigned to the randomly sampled items will inevitably introduce some noise to the model training process, which will then cause loss in recommendation performance. In this paper, we propose a novel lossless federated recommendation method (FedRec++) by allocating some denoising clients (i.e., users) to eliminate the noise in a privacy-aware manner. We further analyse our FedRec++ in terms of security and losslessness, and discuss its generality in the context of existing works. Extensive empirical studies clearly show the effectiveness of our FedRec++ in providing accurate and privacy-aware recommendation without much additional communication cost.

随着联邦机器学习和推荐系统的结合，隐私感知的偏好建模和个性化，出现了一个新的研究分支——联邦推荐系统，旨在以分布式的方式构建推荐模型，即:每个用户都表示为一个分布式客户机，其中他/她的原始评级数据不与服务器或其他客户机共享。请注意，除了用户为某一物品分配的特定评级分数的敏感信息外，用户的评级物品集的信息也应得到很好的保护。最近的一些工作建议为每个用户随机取样一些未评级的项目，然后分配一些虚拟评级，这样服务器就不能在服务器-客户端交互过程中轻松地识别分数和评级项目集。然而，对随机抽样的项目进行虚拟评分，不可避免地会给模型训练过程引入一些噪声，从而导致推荐性能的损失。在本文中，我们提出了一种新的无损联邦推荐方法(FedRec++)，通过分配去噪客户端(即用户)以一种隐私意识的方式去噪。我们进一步分析了我们的FedRec++在安全性和无损方面，并讨论了其在现有工作背景下的通用性。大量的实证研究清楚地表明，我们的FedRec++在提供准确和隐私意识的建议而不需要太多额外的通信成本方面的有效性。

2. Model-sharing Games: Analyzing Federated Learning Under Voluntary Participation.
https://ojs.aaai.org/index.php/AAAI/article/view/16669

Abstract
Federated learning is a setting where agents, each with access to their own data source, combine models learned from local data to create a global model. If agents are drawing their data from different distributions, though, federated learning might produce a biased global model that is not optimal for each agent. This means that agents face  a fundamental question: should they join the global model or stay with their local model? In this work, we show how this situation can be naturally analyzed through the framework of coalitional game theory. 

Motivated by these considerations, we propose the following game:  there are heterogeneous players with  different model parameters  governing their data distribution and different amounts of data they have noisily drawn from their own distribution. Each player's goal is to obtain a model with minimal expected mean squared error (MSE) on their own distribution. They have a choice of fitting a model based solely on their own data, or combining their learned parameters with those of some subset of the other players. Combining models reduces the variance component of their error through access to more data, but increases the bias because of the heterogeneity of distributions. In this work, we derive exact expected MSE values for problems in linear regression and mean estimation. We use these values to analyze the resulting game in the framework of hedonic game theory; we study how players might divide into coalitions, where each set of players within a coalition jointly constructs a single model.   In a case with arbitrarily many players that each have either a "small" or "large" amount of data, we constructively show that there always exists a stable partition of players into coalitions.  

联合学习是一种设置，每个代理都可以访问自己的数据源，将从本地数据学习到的模型组合起来，以创建一个全局模型。但是，如果代理从不同的分布中提取数据，那么联合学习可能会产生一个不适合每个代理的有偏差的全局模型。这意味着代理商面临着一个根本性的问题:他们应该加入全局模式还是保留他们的本地模式?在这项工作中，我们展示了如何通过联盟博弈理论的框架自然地分析这种情况。
基于这些考虑，我们提出了以下的游戏:不同的玩家使用不同的模型参数来控制他们的数据分布，以及他们从自己的分布中获取的不同数量的数据。每个参与者的目标是在他们自己的分布上获得一个具有最小期望均方误差(MSE)的模型。他们可以选择仅仅基于自己的数据去适应一个模型，或者将他们学到的参数与其他玩家的一些子集相结合。通过访问更多的数据，组合模型减少了误差的方差成分，但由于分布的异质性增加了偏差。在这项工作中，我们得到精确的期望MSE值的问题在线性回归和均值估计。我们用这些值在享乐博弈论的框架下分析结果博弈;我们研究参与者如何分裂成联盟，联盟中的每组参与者共同构建一个单一模型。在任意多玩家的情况下，每个玩家都有一个“小”或“大”的数据量，我们建设性地表明，总是存在一个稳定的将玩家划分为联盟的情况。

3. Provably Secure Federated Learning against Malicious Clients.
https://ojs.aaai.org/index.php/AAAI/article/view/16849

Abstract
Federated learning enables clients to collaboratively learn a shared global model without sharing their local training data with a cloud server. However, malicious clients can corrupt the  global  model  to  predict  incorrect  labels  for  testing  examples. Existing defenses against malicious clients leverage Byzantine-robust federated learning methods. However, these methods cannot provably guarantee that the predicted label for  a  testing  example  is  not  affected  by  malicious  clients. We bridge this gap via ensemble federated learning. In particular, given any base federated learning algorithm, we use the algorithm to learn multiple global models, each of which is learnt using a randomly selected subset of clients. When predicting  the  label  of  a  testing  example,  we  take  majority vote  among  the  global  models.  We  show  that  our  ensemble  federated  learning  with  any  base  federated  learning  algorithm is provably secure against malicious clients. Specifically, the label predicted by our ensemble global model for a testing example is provably not affected by a bounded number of malicious clients. Moreover, we show that our derived bound is tight. We evaluate our method on MNIST and Human Activity Recognition datasets. For instance, our method can achieve a certified accuracy of 88% on MNIST when 20 out of 1,000 clients are malicious.

联合学习使客户端能够协作学习一个共享的全局模型，而无需与云服务器共享他们的本地训练数据。然而，恶意的客户端可能会破坏全局模型，从而预测测试示例的不正确标签。现有的针对恶意客户端的防御利用了拜占庭式健壮的联邦学习方法。然而，这些方法不能证明测试示例的预测标签不受恶意客户机的影响。我们通过集成联合学习来弥补这个差距。特别是，给定任何基础联邦学习算法，我们使用该算法来学习多个全局模型，每个全局模型都是使用随机选择的客户子集学习的。当预测测试示例的标签时，我们在全局模型中采用多数投票。我们证明了我们的集成联邦学习与任何基础联邦学习算法对于恶意客户端是安全的。具体来说，我们的集成全局模型预测的测试示例的标签可以证明不受有限数量的恶意客户机的影响。此外，我们证明了导出的界是紧的。我们评估了我们的方法在MNIST和人类活动识别数据集。例如，当1000个客户中有20个是恶意用户时，我们的方法在MNIST上可以达到88%的认证准确率。

4. On the Convergence of Communication-Efficient Local SGD for Federated Learning.
https://ojs.aaai.org/index.php/AAAI/article/view/16920

Abstract
Federated Learning (FL) has attracted increasing attention in recent years. A leading training algorithm in FL is local SGD, which updates the model parameter on each worker and averages model parameters across different workers only once in a while. Although it has fewer communication rounds than the classical parallel SGD, local SGD still has large communication overhead in each communication round for large machine learning models, such as deep neural networks.  To address this issue, we propose a new communication-efficient distributed SGD method,  which can significantly reduce the communication cost by the error-compensated double compression mechanism. Under the non-convex setting, our theoretical results show that our approach has better communication complexity than existing  methods and enjoys the same linear speedup regarding the number of workers as the full-precision local SGD.  Moreover, we  propose a communication-efficient distributed SGD with momentum, which also has better communication complexity than existing methods and enjoys a linear speedup with respect to the number of workers.  At last, extensive experiments are conducted to verify the performance of our proposed two methods. Moreover, we  propose a communication-efficient distributed SGD with momentum to accelerate the convergence, which also has better communication complexity than existing methods and enjoys a linear speedup with respect to the number of workers.  At last, extensive experiments are conducted to verify the performance of our proposed methods.

联邦学习近年来受到越来越多的关注。FL中一个领先的训练算法是局部SGD，它更新每个工人的模型参数，并仅在一段时间内平均不同工人的模型参数。虽然与经典并行SGD相比，局部SGD的通信轮数较少，但对于深度神经网络等大型机器学习模型，局部SGD在每个通信轮中仍有较大的通信开销。为了解决这一问题，我们提出了一种新的通信效率高的分布式SGD方法，该方法通过误码补偿双压缩机制可以显著降低通信成本。在非凸设置下，我们的理论结果表明，我们的方法比现有的方法具有更好的通信复杂度，并具有与全精度局部SGD相同的工人数的线性加速。此外，我们提出了一种具有动量的通信效率的分布式SGD，它比现有的方法具有更好的通信复杂度，并且相对于worker的数量具有线性加速。最后，通过大量的实验验证了所提出的两种方法的性能。此外，我们还提出了一种具有动量的通信效率的分布式SGD来加速收敛，该方法具有比现有方法更好的通信复杂度，并且相对于worker数量具有线性加速。最后，进行了大量的实验来验证我们提出的方法的性能。

5. Personalized Cross-Silo Federated Learning on Non-IID Data.
https://ojs.aaai.org/index.php/AAAI/article/view/16960

Abstract
Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.
				

6. FLAME: Differentially Private Federated Learning in the Shuffle Model.
https://ojs.aaai.org/index.php/AAAI/article/view/17053

Abstract
	Federated Learning (FL) is a promising machine learning paradigm that enables the analyzer to train a model without collecting users' raw data. To ensure users' privacy, differentially private federated learning has been intensively studied. The existing works are mainly based on the curator model or local model of differential privacy. However, both of them have pros and cons. The curator model allows greater accuracy but requires a trusted analyzer.  In the local model where users randomize local data before sending them to the analyzer, a trusted analyzer is not required but the accuracy is limited. In this work, by leveraging the \textit{privacy amplification} effect in the recently proposed shuffle model of differential privacy, we achieve the best of two worlds, i.e., accuracy in the curator model and strong privacy without relying on any trusted party. We first propose an FL framework in the shuffle model and a simple protocol (SS-Simple) extended from existing work. We find that SS-Simple only provides an insufficient privacy amplification effect in FL since the dimension of the model parameter is quite large. To solve this challenge, we propose an enhanced protocol (SS-Double) to increase the privacy amplification effect by subsampling. Furthermore, for boosting the utility when the model size is greater than the user population, we propose an advanced protocol (SS-Topk) with gradient sparsification techniques. We also provide theoretical analysis and numerical evaluations of the privacy amplification of the proposed protocols. Experiments on real-world dataset validate that SS-Topk improves the testing accuracy by 60.7% than the local model based FL. We highlight an observation that SS-Topk improves the accuracy by 33.94\% than the curator model based FL without any trusted party. Compared with non-private FL, our protocol SS-Topk only lose 1.48% accuracy under (2.348, 5e-6)-DP per epoch.
				

7. Game of Gradients: Mitigating Irrelevant Clients in Federated Learning.
https://ojs.aaai.org/index.php/AAAI/article/view/17093

Abstract
	The paradigm of Federated learning (FL) deals with multiple clients participating in collaborative training of a machine learning model under the orchestration of a central server. In this setup, each client’s data is private to itself and is not transferable to other clients or the server. Though FL paradigm has received significant interest recently from the research community, the problem of selecting the relevant clients w.r.t. the central server's learning objective is under-explored. We refer to these problems as Federated Relevant Client Selection (FRCS). Because the server doesn't have explicit control over the nature of data possessed by each client, the problem of selecting relevant clients is significantly complex in FL settings.
In this paper, we resolve important and related FRCS problems viz., selecting clients with relevant data, detecting clients that possess data relevant to a particular target label, and rectifying corrupted data samples of individual clients. We follow a principled approach to address the above FRCS problems and develop a new federated learning method using the Shapley value concept from cooperative game theory. Towards this end, we propose a cooperative game involving the gradients shared by the clients. Using this game, we compute Shapley values of clients and then present Shapley value based Federated Averaging (S-FedAvg) algorithm that empowers the server to select relevant clients with high probability. S-FedAvg turns out to be critical in designing specific algorithms to address the FRCS problems. We finally conduct a thorough empirical analysis on image classification and speech recognition tasks to show the superior performance of S-FedAvg than the baselines in the context of supervised federated learning settings.
				

8. Defending against Backdoors in Federated Learning with Robust Learning Rate.
https://ojs.aaai.org/index.php/AAAI/article/view/17118

Abstract
	Federated learning (FL) allows a set of agents to collaboratively train a model without sharing their potentially sensitive data. This makes FL suitable for privacy-preserving applications. At the same time, FL is susceptible to adversarial attacks due to decentralized and unvetted data. One important line of attacks against FL is the backdoor attacks. In a backdoor attack, an adversary tries to embed a backdoor functionality to the model during training that can later be activated to cause a desired misclassification. To prevent backdoor attacks, we propose a lightweight defense that requires minimal change to the FL protocol. At a high level, our defense is based on carefully adjusting the aggregation server's learning rate, per dimension and per round, based on the sign information of agents' updates. We first conjecture the necessary steps to carry a successful backdoor attack in FL setting, and then, explicitly formulate the defense based on our conjecture. Through experiments, we provide empirical evidence that supports our conjecture, and we test our defense against backdoor attacks under different settings. We observe that either backdoor is completely eliminated, or its accuracy is significantly reduced. Overall, our experiments suggest that our defense significantly outperforms some of the recently proposed defenses in the literature. We achieve this by having minimal influence over the accuracy of the trained models. In addition, we also provide convergence rate analysis for our proposed scheme.
				

9. Federated Multi-Armed Bandits.
https://ojs.aaai.org/index.php/AAAI/article/view/17156

Abstract
	Federated multi-armed bandits (FMAB) is a new bandit paradigm that parallels the federated learning (FL) framework in supervised learning. It is inspired by practical applications in cognitive radio and recommender systems, and enjoys features that are analogous to FL. This paper proposes a general framework of FMAB and then studies two specific federated bandit models. We first study the approximate model where the heterogeneous local models are random realizations of the global model from an unknown distribution. This model introduces a new uncertainty of client sampling, as the global model may not be reliably learned even if the finite local models are perfectly known. Furthermore, this uncertainty cannot be quantified a priori without knowledge of the suboptimality gap. We solve the approximate model by proposing Federated Double UCB (Fed2-UCB), which constructs a novel “double UCB” principle accounting for uncertainties from both arm and client sampling. We show that gradually admitting new clients is critical in achieving an O(log(T)) regret while explicitly considering the communication loss. The exact model, where the global bandit model is the exact average of heterogeneous local models, is then studied as a special case. We show that, somewhat surprisingly, the order-optimal regret can be achieved independent of the number of clients with a careful choice of the update periodicity. Experiments using both synthetic and real-world datasets corroborate the theoretical analysis and demonstrate the effectiveness and efficiency of the proposed algorithms.
				

10. Addressing Class Imbalance in Federated Learning.
https://ojs.aaai.org/index.php/AAAI/article/view/17219

Abstract
	Federated learning (FL) is a promising approach for training decentralized data located on local client devices while improving efficiency and privacy. However, the distribution and quantity of the training data on the clients' side may lead to significant challenges such as class imbalance and non-IID (non-independent and identically distributed) data, which could greatly impact the performance of the common model. While much effort has been devoted to helping FL models converge when encountering non-IID data, the imbalance issue has not been sufficiently addressed. In particular, as FL training is executed by exchanging gradients in an encrypted form, the training data is not completely observable to either clients or server, and previous methods for class imbalance do not perform well for FL. Therefore, it is crucial to design new methods for detecting class imbalance in FL and mitigating its impact. In this work, we propose a monitoring scheme that can infer the composition of training data for each FL round, and design a new loss function -- Ratio Loss to mitigate the impact of the imbalance. Our experiments demonstrate the importance of acknowledging class imbalance and taking measures as early as possible in FL training, and the effectiveness of our method in mitigating the impact. Our method is shown to significantly outperform previous methods, while maintaining client privacy.
				

11. Federated Block Coordinate Descent Scheme for Learning Global and Personalized Models.
https://ojs.aaai.org/index.php/AAAI/article/view/17240

Abstract
	In federated learning, models are learned from users’ data that are held private in their edge devices, by aggregating them in the service provider’s “cloud” to obtain a global model. Such global model is of great commercial value in, e.g., improving the customers’ experience. In this paper we focus on two possible areas of improvement of the state of the art. First, we take the difference between user habits into account and propose a quadratic penalty-based formulation, for efficient learning of the global model that allows to personalize local models. Second, we address the latency issue associated with the heterogeneous training time on edge devices, by exploiting a hierarchical structure modeling communication not only between the cloud and edge devices, but also within the cloud. Specifically, we devise a tailored block coordinate descent-based computation scheme, accompanied with communication protocols for both the synchronous and asynchronous cloud settings. We characterize the theoretical convergence rate of the algorithm, and provide a variant that performs empirically better. We also prove that the asynchronous protocol, inspired by multi-agent consensus technique, has the potential for large gains in latency compared to a synchronous setting when the edge-device updates are intermittent. Finally, experimental results are provided that corroborate not only the theory, but also show that the system leads to faster convergence for personalized models on the edge devices, compared to the state of the art.
				

12. Toward Understanding the Influence of Individual Clients in Federated Learning.
https://ojs.aaai.org/index.php/AAAI/article/view/17263

Abstract
	Federated learning allows mobile clients to jointly train a global model without sending their private data to a central server. Extensive works have studied the performance guarantee of the global model, however, it is still unclear how each individual client influences the collaborative training process. In this work, we defined a new notion, called {\em Fed-Influence}, to quantify this influence over the model parameters, and proposed an effective and efficient algorithm to estimate this metric. In particular, our design satisfies several desirable properties: (1) it requires neither retraining nor retracing, adding only linear computational overhead to clients and the server; (2) it strictly maintains the tenets of federated learning, without revealing any client's local private data; and (3) it works well on both convex and non-convex loss functions, and does not require the final model to be optimal. Empirical results on a synthetic dataset and the FEMNIST dataset demonstrate that our estimation method can approximate Fed-Influence with small bias. Further, we show an application of Fed-Influence in model debugging.
				

13. Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning.
https://ojs.aaai.org/index.php/AAAI/article/view/17291

Abstract
	Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly, data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods and systems.
				

14. Secure Bilevel Asynchronous Vertical Federated Learning with Backward Updating.
https://ojs.aaai.org/index.php/AAAI/article/view/17301

Abstract
	Vertical federated learning (VFL) attracts increasing attention due to the emerging demands of multi-party collaborative modeling and concerns of privacy leakage. In the real VFL applications, usually only one or partial parties hold labels, which makes it challenging for all parties to collaboratively learn the model without privacy leakage. Meanwhile, most existing VFL algorithms are trapped in the synchronous computations, which leads to inefficiency in their real-world applications. To address these challenging problems, we propose a novel VFL framework integrated with new backward updating mechanism and bilevel asynchronous parallel architecture (VFB^2), under which three new algorithms, including VFB^2-SGD, -SVRG, and -SAGA, are proposed. We derive the theoretical results of the convergence rates of these three algorithms under both strongly convex and nonconvex conditions. We also prove the security of VFB^2 under semi-honest threat models. Extensive experiments on benchmark datasets demonstrate that our algorithms are efficient, scalable, and lossless.
				

15. A Serverless Approach to Federated Learning Infrastructure Oriented for IoT/Edge Data Sources (Student Abstract).
https://ojs.aaai.org/index.php/AAAI/article/view/17870

Abstract
	The paper proposes a Serverless and Mobile relay based architecture for a highly scalable Federated Learning system for low power IoT and Edge Devices. The aim is an easily deployable infrastructure on a public cloud platform by the end user and democratize the use of federated learning.
				

