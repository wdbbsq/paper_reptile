1. Federated Learning Based on Dynamic Regularization.
https://openreview.net/forum?id=B7v4QMR6Z9w

We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.

我们提出了一种新的联合学习方法，用于分布式训练神经网络模型，其中服务器协调在每轮随机选择的设备子集之间的合作。我们主要从通信的角度来看待联邦学习问题，并允许更多的设备级计算来节省传输成本。我们指出了一个基本的困境，即本地设备水平的经验损失的最小值与全球经验损失的最小值不一致。与以往的研究不同的是，我们提出了一种动态正则化算法，在每一轮中针对每个设备，从而在极限下使全局和设备解对齐。我们通过真实数据和合成数据的实证结果以及分析结果证明，在凸和非凸设置下，我们的方案可以实现高效的培训，同时完全不考虑设备异质性，并对大量设备、部分参与和不平衡数据具有鲁棒性。

2. Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms.
https://openreview.net/forum?id=GFsU8a0sGB

Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm—federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.

联邦学习通常作为一个优化问题来处理，其目标是通过在拥有本地数据并指定全局目标的不同部分的客户端设备上分布计算来最小化全局损失函数。我们提出了另一种观点，并将联邦学习表述为一个后验推理问题，其中的目标是通过让每个客户端设备推断其本地数据的后验来推断一个全局后验分布。虽然精确推断通常很难，但这种视角提供了一种原则性的方法，可以在联邦设置中搜索全局最优设置。此外，从联邦二次目标的分析出发，提出了一种计算效率高、通信效率高的近似后验推理算法——联邦后验平均算法。我们的算法使用MCMC来对客户端进行局部后验模式的近似推断，并有效地将其统计信息传递给服务器，后者使用它们来优化后验模式的全局估计。最后，我们证明了FedPA推广了联邦平均(FedAvg)，同样可以从自适应优化器中受益，并在四个现实且具有挑战性的基准上产生最先进的结果，收敛速度更快，达到更好的优化。

3. Adaptive Federated Optimization.
https://openreview.net/forum?id=LkFG3lB13U5

Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and  Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.

联邦学习是一种分布式的机器学习模式，在这种模式下，大量的客户端与一个中央服务器协调学习一个模型，而不共享他们自己的训练数据。标准联邦优化方法，如联邦平均(FedAvg)，通常难以调优，并表现出不良的收敛行为。在非联邦设置中，自适应优化方法在解决此类问题方面取得了显著的成功。在这项工作中，我们提出了自适应优化器的联邦版本，包括Adagrad、Adam和Yogi，并分析了它们在一般非凸设置的异构数据存在下的收敛性。我们的研究结果强调了客户端异构性和通信效率之间的相互作用。我们还对这些方法进行了大量的实验，并表明使用自适应优化器可以显著提高联邦学习的性能。

4. Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning.
https://openreview.net/forum?id=jDdzh5ul-d

Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data.  FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL.  In this paper, we show that the answer is affirmative.  Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate $\mathcal{O}(\frac{1}{\sqrt{mKT}} + \frac{1}{T})$ for full worker participation and a convergence rate $\mathcal{O}(\frac{\sqrt{K}}{\sqrt{nT}} + \frac{1}{T})$ for partial worker participation, where $K$ is the number of local steps, $T$ is the number of total communication rounds, $m$ is the total worker number and $n$ is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to $T/m$ in full worker participation. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.

联合学习(FL)是一种分布式的机器学习体系结构，它利用大量的工作人员共同学习一个具有分散数据的模型。FL由于其数据隐私保护、通信效率和训练收敛的线性加速(即收敛性能随工作人员数量的增加而线性增加)，近年来受到越来越多的关注。然而，现有的关于线性加速收敛的研究仅局限于假设i.i.d.数据集跨工人和/或全工人参与，这两种假设在实践中都很少成立。到目前为止，在非i.i.d条件下是否能实现收敛的线性加速仍然是一个悬而未决的问题。在这篇论文中，我们证明了答案是肯定的。具体地说，我们证明了联邦平均(FedAvg)算法(双边学习率)对非i.i.d。数据集在非凸的设置达到收敛速度\美元mathcal {O}(\压裂{1}{\ sqrt {mKT}} + \压裂{1}{T})全职工人参与和收敛速度美元\ mathcal {O}(\压裂{\ sqrt {K}} {\ sqrt {nT}} + \压裂{1}{T})美元部分工人参与,其中K美元是当地的步数,T是美元的数量总沟通,$m$为总员工数，$n$为部分员工参与时，一轮沟通中的员工数。我们的研究结果还表明，FL中的局部步数有助于收敛，并表明在全工人参与下，最大局部步数可以提高到$T/m$。我们在MNIST和CIFAR-10上进行了大量的实验来验证我们的理论结果。

5. Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning.
https://openreview.net/forum?id=ce6CFXBh30h

While existing federated learning approaches mostly require that clients have fully-labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi-Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning.

虽然现有的联合学习方法大多要求客户端有完整标记的数据来进行训练，但在现实的设置中，客户端获得的数据往往没有任何伴随的标签。这种标注的不足可能是由于标注成本高，也可能是由于对专家知识的要求导致标注困难。因此，每个客户机上的私有数据要么是部分标记的，要么是完全不标记的，因为标记的数据只在服务器上可用，这就导致了一个新的实际的联邦学习问题，即联邦半监督学习(FSSL)。在这项工作中，我们研究了基于标记数据位置的FSSL的两个基本场景。第一个场景考虑的是一种传统的情况，即客户端同时拥有已标记和未标记的数据(label -at-client)，而第二个场景考虑的是一种更具挑战性的情况，即已标记的数据仅在服务器上可用(label -at-server)。然后，我们提出了一种新的方法来解决这些问题，我们称之为联邦匹配(federmatch)。FedMatch改进了联邦学习和半监督学习方法的朴素组合，对标记数据和非标记数据进行了新的客户端间一致性损失和参数分解。通过在两种不同场景下对我们的方法进行广泛的实验验证，我们表明我们的方法优于局部半监督学习和基线联合学习与半监督学习。

6. FedBN: Federated Learning on Non-IID Features via Local Batch Normalization.
https://openreview.net/forum?id=6YEQUn0QICG

The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at https://github.com/med-air/FedBN.

新兴的联邦学习范式(FL)致力于在网络边缘实现深度模型的协同训练，而不需要集中聚合原始数据，从而提高数据隐私性。在大多数情况下，跨本地客户端独立且相同分布的样本的假设并不适用于联邦学习设置。在此设置下，根据数据分布的不同，神经网络的训练性能可能会有很大的差异，甚至会影响训练的收敛性。之前的大部分工作都集中在标签的分配或客户转移的差异上。与这些设置不同，我们解决了FL的一个重要问题，例如，医疗成像中的不同扫描仪/传感器，自动驾驶中的不同场景分布(高速公路vs.城市)，本地客户端存储的示例与其他客户端相比具有不同的分布，我们将其表示为feature shift non-iid。在本研究中，我们提出了一种有效的方法，使用局部批量归一化来缓解模型平均前的特征转移。在我们的大量实验中，最终得到的方案被称为FedBN，其性能优于经典的FedAvg和最先进的非iid数据(FedProx)。结果表明，在简化条件下，FedBN的收敛速度比FedAvg快。代码可以在https://github.com/med-air/FedBN上找到。

7. FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning.
https://openreview.net/forum?id=dgtpE6gKjHn

Federated learning aims to collaboratively train a strong global model by accessing users' locally trained models but not their own data. A crucial step is therefore to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. In this paper, we propose a novel aggregation algorithm named FedBE, which takes a Bayesian inference perspective by sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. We show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. Our empirical studies validate FedBE's superior performance, especially when users' data are not i.i.d. and when the neural networks go deeper. Moreover, FedBE is compatible with recent efforts in regularizing users' model training, making it an easily applicable module: you only need to replace the aggregation method but leave other parts of your federated learning algorithm intact.

联合学习的目的是通过访问用户本地训练的模型而不是他们自己的数据来协作训练一个强大的全局模型。因此，关键的一步是将局部模型聚合为全局模型，当用户没有i.i.d时，这就很有挑战性了。数据。本文提出了一种新的聚合算法FedBE，该算法采用贝叶斯推理的视角，对高质量的全局模型进行采样，并将其通过贝叶斯模型集成(Bayesian model Ensemble)进行组合，从而实现鲁棒聚合。我们证明了一个有效的模型分布可以通过简单地拟合高斯或狄利克雷分布到局部模型。我们的实证研究验证了FedBE的卓越性能，特别是当用户数据不是身份信息时，以及当神经网络深入时。此外，FedBE与最近正则化用户模型训练的努力兼容，使其成为一个易于应用的模块:您只需要替换聚合方法，但保留联邦学习算法的其他部分。

8. FedMix: Approximation of Mixup under Mean Augmented Federated Learning.
https://openreview.net/forum?id=Ogga20D2HO-

Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer a performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, \emph{Mean Augmented Federated Learning (MAFL)}, where clients send and receive \emph{averaged} local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named \emph{FedMix}, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.

联合学习(FL)允许边缘设备集体学习一个模型，而无需在每个设备中直接共享数据，从而保护隐私，消除了全局存储数据的需要。虽然在假设独立和同分布(iid)本地数据的情况下会有很好的结果，但随着客户机间本地数据的异构性增加，当前最先进的算法的性能会下降。为了解决这个问题，我们提出了一个简单的框架，平均增强联合学习(MAFL)}，其中客户端发送和接收平均的本地数据，以满足目标应用程序的隐私要求。在我们的框架下，我们提出了一种新的增强算法，名为\emph{FedMix}，它的灵感来自于一种惊人而简单的数据增强方法，Mixup，但不需要本地原始数据直接在设备之间共享。在高度非iid联邦设置下，与传统算法相比，我们的方法在FL的标准基准数据集上显示了极大的改进性能。

9. HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients.
https://openreview.net/forum?id=TNkPBBYFkXg

Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.

联邦学习(FL)是一种在大量可能异构的客户端(如手机和物联网设备)上分布的私有数据上训练机器学习模型的方法。在这项工作中，我们提出了一个新的名为HeteroFL的联合学习框架，以处理具有非常不同的计算和通信能力的异构客户端。我们的解决方案能够在训练具有不同计算复杂性的异构局部模型的同时，仍然生成一个全局推理模型。我们的方法第一次挑战了现有工作的基本假设，即本地模型必须与全局模型共享相同的架构。我们展示了几种增强FL训练的策略，并进行了广泛的经验评估，包括在三个数据集上的三个模型体系结构的五个计算复杂度级别。我们证明了根据客户端能力自适应分配子网既能提高计算效率，又能提高通信效率。

10. Personalized Federated Learning with First Order Model Optimization.
https://openreview.net/forum?id=ehJqJQk9cw

While federated learning traditionally aims to train a single global model across decentralized local datasets, one model may not always be ideal for all participating clients. Here we propose an alternative, where each client only federates with other relevant clients to obtain a stronger model per client-specific objectives. To achieve this personalization, rather than computing a single model average with constant weights for the entire federation as in traditional FL, we efficiently calculate optimal weighted model combinations for each client, based on figuring out how much a client can benefit from another's model. We do not assume knowledge of any underlying data distributions or client similarities, and allow each client to optimize for arbitrary target distributions of interest, enabling greater flexibility for personalization. We evaluate and characterize our method on a variety of federated settings, datasets, and degrees of local data heterogeneity. Our method outperforms existing alternatives, while also enabling new features for personalized FL such as transfer outside of local data distributions.

虽然联邦学习传统上旨在跨分散的本地数据集训练单个全局模型，但一个模型可能并不总是适合所有参与的客户。在这里，我们提出了一种替代方案，即每个客户机仅与其他相关客户机联合，以获得针对客户机特定目标的更强大的模型。为了实现这种个性化，与传统的FL中计算整个联邦具有恒定权重的单个模型平均值不同，我们根据计算一个客户可以从另一个客户的模型中获益多少，为每个客户有效地计算最优加权模型组合。我们不假设了解任何底层数据分布或客户端相似性，并允许每个客户端针对任意感兴趣的目标分布进行优化，从而为个性化提供更大的灵活性。我们在各种联邦设置、数据集和本地数据异构程度上评估和描述我们的方法。我们的方法优于现有的替代方法，同时也为个性化FL提供了新的功能，比如在本地数据分布之外进行传输。
